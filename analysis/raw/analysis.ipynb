{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats\n",
    "Statistics about downloaded and processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the relative path or the absolute path pointing to the directory in which datasets have been downloaded\n",
    "folder = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_datasets = list()  # completely downloaded\n",
    "partial_datasets = list()  # not completely downloaded / parsed (at least 1 valid file)\n",
    "empty_datasets = list()  # only metadata for these datasets\n",
    "not_processed = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan the directory containing the downloaded datasets\n",
    "datasets = sorted(os.listdir(folder), key=lambda i: int(i))\n",
    "total_datasets = len(datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SIZE_LIMIT = 200 * 1024 * 1024  # 200 MB\n",
    "\n",
    "\n",
    "def is_file_larger_than_size_limit(filepath: str) -> bool:\n",
    "    size = os.path.getsize(str(filepath))\n",
    "    return int(size) >= int(SIZE_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDF_SUFFIXES = [\"rdf\", \"ttl\", \"owl\", \"n3\", \"nt\", \"jsonld\", \"nq\", \"trig\", \"trix\"]\n",
    "\n",
    "\n",
    "def check_if_file_name_is_rdf(name: str) -> bool:\n",
    "    return name.split(\".\")[-1] in RDF_SUFFIXES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import magic\n",
    "\n",
    "\n",
    "def is_html(filepath: str) -> bool:\n",
    "    mt = magic.from_file(filepath).lower()\n",
    "    if \"html\" in mt:\n",
    "        return True\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        try:\n",
    "            return \"<!doctype html\" in f.read().lower()\n",
    "        except Exception:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def is_json(filepath: str) -> bool:\n",
    "    with open(filepath, \"r\") as f:\n",
    "        try:\n",
    "            json.load(f)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the processing status for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    EMPTY = 0\n",
    "    NOT_PROCESSED = 1\n",
    "    PARTIAL = 2\n",
    "    COMPLETE = 3\n",
    "\n",
    "\n",
    "def analyze_dataset(dataset_path) -> DatasetType:\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        keys = metadata.keys()\n",
    "\n",
    "        # check if the dataset has been mined through the data extractor\n",
    "        if not \"unused_files\" in keys:\n",
    "            return DatasetType.NOT_PROCESSED\n",
    "\n",
    "        # check if the dataset has been downloaded completely\n",
    "        completely_downloaded = False\n",
    "        if \"failed_download_urls\" in keys:\n",
    "            completely_downloaded = len(metadata[\"failed_download_urls\"]) == 0\n",
    "\n",
    "        # check if the file dataset contains at least one file that has been parsed\n",
    "        contains_a_valid_file = len(metadata[\"used_files\"]) > 0\n",
    "\n",
    "        # check if the dataset has some files that have not been parsed or has thrown errors while parsing\n",
    "        error_while_parsing = len(metadata[\"unused_files\"]) == 0\n",
    "\n",
    "        \"\"\" \n",
    "        A dataset is complete only if all these conditions are satisfied:\n",
    "        1) has been completely downloaded\n",
    "        2) contains at least one valid file (>0)\n",
    "        3) no file has generated error while parsing\n",
    "        \"\"\"\n",
    "\n",
    "        if completely_downloaded and contains_a_valid_file and not error_while_parsing:\n",
    "            return DatasetType.COMPLETE\n",
    "\n",
    "        \"\"\"\n",
    "        A dataset is partial if:\n",
    "        1) contains at least one valid file (>0)\n",
    "        2) some files may not have been downloaded\n",
    "        3) some files may have generated errors or not being the correct type to be used\n",
    "        \"\"\"\n",
    "\n",
    "        if contains_a_valid_file:\n",
    "            return DatasetType.PARTIAL\n",
    "\n",
    "        \"\"\"\n",
    "        If a dataset doesn't contain any file\n",
    "        \"\"\"\n",
    "        return DatasetType.EMPTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    metadata_file_path = f\"{folder}/{dataset}/metadata.json\"\n",
    "\n",
    "    res = analyze_dataset(metadata_file_path)\n",
    "\n",
    "    if res == DatasetType.COMPLETE:\n",
    "        complete_datasets.append(dataset)\n",
    "\n",
    "    if res == DatasetType.PARTIAL:\n",
    "        partial_datasets.append(dataset)\n",
    "\n",
    "    if res == DatasetType.EMPTY:\n",
    "        empty_datasets.append(dataset)\n",
    "\n",
    "    if res == DatasetType.NOT_PROCESSED:\n",
    "        not_processed.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of datasets: {total_datasets}\")\n",
    "print(f\"Complete datasets: {len(complete_datasets)}\")   # completely downloaded and parsed\n",
    "print(f\"Partial datasets: {len(partial_datasets)}\")\n",
    "print(f\"Empty datasets: {len(empty_datasets)}\")\n",
    "print(f\"Not processed datasets: {len(not_processed)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List datasets with unused file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_with_unused_files = list()\n",
    "\n",
    "for dataset in datasets:\n",
    "    metadata_file_path = f\"{folder}/{dataset}/metadata.json\"\n",
    "\n",
    "    with open(metadata_file_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        keys = metadata.keys()\n",
    "\n",
    "        if \"unused_files\" in keys and len(metadata[\"unused_files\"]) > 0:\n",
    "            unparsable_rdf = list()\n",
    "            unparsable_other = list()\n",
    "\n",
    "            for file in metadata[\"unused_files\"]:\n",
    "                file_with_path = f\"{folder}/{dataset}/{file}\"\n",
    "\n",
    "                if check_if_file_name_is_rdf(file):\n",
    "                    unparsable_rdf.append(file)\n",
    "                else:\n",
    "                    unparsable_other.append(file)\n",
    "            \n",
    "            datasets_with_unused_files.append([dataset, unparsable_rdf, unparsable_other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "markdown_table = \"\"\"\n",
    "| Dataset ID | RDF not parsable | Other not parsable |\n",
    "| --- | --- | --- |\n",
    "\"\"\"\n",
    "\n",
    "for d in datasets_with_unused_files:\n",
    "    markdown_table += (\"| {} | {} | {} |\\n\".format(d[0], str(d[1]), str(d[2])))\n",
    "\n",
    "display(Markdown(markdown_table))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing unused files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_files_unused = list()       # contains the path of unused RDF files\n",
    "other_files_unused = list()     # contains the path on unused NON RDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_used_files = 0\n",
    "\n",
    "for dataset in datasets:\n",
    "    metadata_file_path = f\"{folder}/{dataset}/metadata.json\"\n",
    "\n",
    "    with open(metadata_file_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        keys = metadata.keys()\n",
    "\n",
    "        if \"used_files\" in keys and len(metadata[\"used_files\"]) > 0:\n",
    "            total_used_files += len(metadata[\"used_files\"])\n",
    "\n",
    "        if \"unused_files\" in keys and len(metadata[\"unused_files\"]) > 0:\n",
    "\n",
    "            for file in metadata[\"unused_files\"]:\n",
    "                file_with_path = f\"{folder}/{dataset}/{file}\"\n",
    "\n",
    "                if check_if_file_name_is_rdf(file):\n",
    "                    rdf_files_unused.append(file_with_path)\n",
    "                else:\n",
    "                    other_files_unused.append(file_with_path)\n",
    "\n",
    "print(f\"Total used files: {total_used_files}\")\n",
    "print(f\"RDF but unusable files: {len(rdf_files_unused)}\")\n",
    "print(f\"NON RDF unusable files: {len(other_files_unused)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files with RDF extension that are not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_but_actually_html = list()\n",
    "rdf_but_actually_json = list()\n",
    "rdf_but_too_large = list()\n",
    "rdf_unparsable = list()\n",
    "\n",
    "for file in rdf_files_unused:\n",
    "    if os.path.isfile(file):\n",
    "        if is_file_larger_than_size_limit(file):\n",
    "            rdf_but_too_large.append(file)\n",
    "        elif is_html(file):\n",
    "            rdf_but_actually_html.append(file)\n",
    "        elif is_json(file):\n",
    "            rdf_but_actually_json.append(file)\n",
    "        else:\n",
    "            rdf_unparsable.append(file)\n",
    "\n",
    "print(f\"RDF but actually HTML: {len(rdf_but_actually_html)}\")\n",
    "print(f\"RDF but actually JSON: {len(rdf_but_actually_json)}\")\n",
    "print(f\"RDF but too large: {len(rdf_but_too_large)}\")\n",
    "print(f\"RDF with syntax error: {len(rdf_unparsable)}\")\n",
    "\n",
    "assert len(rdf_files_unused) == len(rdf_but_actually_html) + len(rdf_but_actually_json) + len(rdf_but_too_large) + len(rdf_unparsable)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of unused - NON RDF files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import tarfile\n",
    "import magic\n",
    "\n",
    "\n",
    "def is_gz_file(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return f.read(2) == b\"\\x1f\\x8b\"\n",
    "\n",
    "\n",
    "def is_bz2_file(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return f.read(3) == b\"\\x42\\x5a\\x68\"\n",
    "\n",
    "\n",
    "def maybe_rdf(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        try:\n",
    "            return \"<rdf:RDF\" in f.read()\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "def maybe_ttl(filepath: str) -> bool:\n",
    "    with open(filepath, \"r\") as f:\n",
    "        try:\n",
    "            return \"@prefix\" in f.read().lower()\n",
    "        except Exception:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archives = list()\n",
    "probably_json = list()\n",
    "probably_html = list()\n",
    "cast_to_rdf = list()\n",
    "cast_to_ttl = list()\n",
    "other = list()\n",
    "\n",
    "for file in other_files_unused:\n",
    "    if (\n",
    "        tarfile.is_tarfile(file)\n",
    "        or zipfile.is_zipfile(file)\n",
    "        or is_gz_file(file)\n",
    "        or is_bz2_file(file)\n",
    "    ):\n",
    "        archives.append(file)\n",
    "\n",
    "    elif is_html(file):\n",
    "        probably_html.append(file)\n",
    "\n",
    "    elif is_json(file):\n",
    "        probably_json.append(file)\n",
    "\n",
    "    elif maybe_rdf(file):\n",
    "        cast_to_rdf.append(file)\n",
    "\n",
    "    elif maybe_ttl(file):\n",
    "        cast_to_ttl.append(file)\n",
    "\n",
    "    else:\n",
    "        other.append(file)\n",
    "\n",
    "\n",
    "print(f\"Total number of files recognized as archives: {len(archives)}\")\n",
    "print(f\"Total number of files that probably are HTML documents: {len(probably_html)}\")\n",
    "print(f\"Total number of files that probably are JSON documents: {len(probably_json)}\")\n",
    "print(f\"Total number of files that probably can be casted to RDF: {len(cast_to_rdf)}\")\n",
    "print(f\"Total number of files that probably can be casted to TTL: {len(cast_to_ttl)}\")\n",
    "print(f\"Total number of unknown files: {len(other)}\")\n",
    "\n",
    "assert len(other_files_unused) == len(archives) + len(probably_html) + len(probably_json) + len(cast_to_rdf) + len(cast_to_ttl) + len(other)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final output of the analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files that are going to be deleted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files that are HTML can be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_but_actually_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probably_html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files that are JSON can be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_but_actually_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probably_json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files that needs to be processed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files that probably are RDF but needs to be renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_to_rdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files that probably are TTL but needs to be renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_to_ttl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files that are archives and needs to be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files that needs to be manually processed because they are too big to be processed by RDFLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_but_too_large"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files that needs to be processed manually to assign them an extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in other:\n",
    "    print(f\"{file:<50} | {magic.from_file(file)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDF files that needs to be processed as plain text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_unparsable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
