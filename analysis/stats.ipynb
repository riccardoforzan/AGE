{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats\n",
    "Statistics about downloaded and processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the relative path or the absolute path pointing to the directory in which datasets have been downloaded\n",
    "folder = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_datasets = list()  # completely downloaded\n",
    "partial_datasets = list()  # not completely downloaded / parsed (at least 1 valid file)\n",
    "empty_datasets = list()  # only metadata for these datasets\n",
    "not_processed = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan the directory containing the downloaded datasets\n",
    "datasets = sorted(os.listdir(folder), key=lambda i: int(i))\n",
    "total_datasets = len(datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check datasets with more than 1 file downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_with_more_than_one_file_downloaded = list()\n",
    "\n",
    "for dataset in datasets:\n",
    "    metadata_file_path = f\"{folder}/{dataset}/metadata.json\"\n",
    "\n",
    "    with open(metadata_file_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        if(len(metadata[\"downloaded_urls\"]) > 1 ):\n",
    "            datasets_with_more_than_one_file_downloaded.append(f\"{folder}/{dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Datasets with more than one file downloaded: {len(datasets_with_more_than_one_file_downloaded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets_with_more_than_one_file_downloaded:\n",
    "    print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the processing status for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetType(Enum):\n",
    "    EMPTY = 0\n",
    "    NOT_PROCESSED = 1\n",
    "    PARTIAL = 2\n",
    "    COMPLETE = 3\n",
    "\n",
    "\n",
    "def analyze_dataset(dataset_path) -> DatasetType:\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        keys = metadata.keys()\n",
    "\n",
    "        # check if the dataset has been mined through the data extractor\n",
    "        if not \"unused_files\" in keys:\n",
    "            return DatasetType.NOT_PROCESSED\n",
    "\n",
    "        # check if the dataset has been downloaded completely\n",
    "        completely_downloaded = False\n",
    "        if \"failed_download_urls\" in keys:\n",
    "            completely_downloaded = len(metadata[\"failed_download_urls\"]) == 0\n",
    "\n",
    "        # check if the file dataset contains at least one file that has been parsed\n",
    "        contains_a_valid_file = len(metadata[\"used_files\"]) > 0\n",
    "\n",
    "        # check if the dataset has some files that have not been parsed or has thrown errors while parsing\n",
    "        error_while_parsing = len(metadata[\"unused_files\"]) == 0\n",
    "\n",
    "        \"\"\" \n",
    "        A dataset is complete only if all these conditions are satisfied:\n",
    "        1) has been completely downloaded\n",
    "        2) contains at least one valid file (>0)\n",
    "        3) no file has generated error while parsing\n",
    "        \"\"\"\n",
    "\n",
    "        if completely_downloaded and contains_a_valid_file and not error_while_parsing:\n",
    "            return DatasetType.COMPLETE\n",
    "\n",
    "        \"\"\"\n",
    "        A dataset is partial if:\n",
    "        1) contains at least one valid file (>0)\n",
    "        2) some files may not have been downloaded\n",
    "        3) some files may have generated errors or not being the correct type to be used\n",
    "        \"\"\"\n",
    "\n",
    "        if contains_a_valid_file:\n",
    "            return DatasetType.PARTIAL\n",
    "\n",
    "        \"\"\"\n",
    "        If a dataset doesn't contain any file\n",
    "        \"\"\"\n",
    "        return DatasetType.EMPTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    metadata_file_path = f\"{folder}/{dataset}/metadata.json\"\n",
    "\n",
    "    res = analyze_dataset(metadata_file_path)\n",
    "\n",
    "    if res == DatasetType.COMPLETE:\n",
    "        complete_datasets.append(dataset)\n",
    "\n",
    "    if res == DatasetType.PARTIAL:\n",
    "        partial_datasets.append(dataset)\n",
    "\n",
    "    if res == DatasetType.EMPTY:\n",
    "        empty_datasets.append(dataset)\n",
    "\n",
    "    if res == DatasetType.NOT_PROCESSED:\n",
    "        not_processed.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of datasets: {total_datasets}\")\n",
    "print(f\"Complete datasets: {len(complete_datasets)}\")   # completely downloaded and parsed\n",
    "print(f\"Partial datasets: {len(partial_datasets)}\")\n",
    "print(f\"Empty datasets: {len(empty_datasets)}\")\n",
    "print(f\"Not processed datasets: {len(not_processed)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing unused files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files that are too big to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files that are too big (>100MB) that have not been analyzed\n",
    "\n",
    "SIZE_LIMIT = 100 * 1024 * 1024  # 100 MB\n",
    "\n",
    "EXCLUDE = [\"metadata.json\"]     # generated files, that should be excluded because usually are not target for extraction\n",
    "\n",
    "file_list = list()\n",
    "\n",
    "for path, subdirs, files in os.walk(folder):\n",
    "    for name in files:\n",
    "        if not name in EXCLUDE:\n",
    "            file_list.append(os.path.join(path, name))\n",
    "\n",
    "for i in file_list:\n",
    "    # Getting the size in a variable\n",
    "    size = os.path.getsize(str(i))\n",
    "\n",
    "    # Print the files that meet the condition\n",
    "    if int(size) >= int(SIZE_LIMIT):\n",
    "        print(str(i) + \" is: \" + str(size >> 20) + \"MB\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files that potentially can be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDF_SUFFIXES = [\"rdf\", \"ttl\", \"owl\", \"n3\", \"nt\", \"jsonld\"]\n",
    "\n",
    "\n",
    "def check_if_file_name_is_rdf(name: str) -> bool:\n",
    "    return name.split(\".\")[-1] in RDF_SUFFIXES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_with_unused_files = list()\n",
    "\n",
    "for dataset in datasets:\n",
    "    metadata_file_path = f\"{folder}/{dataset}/metadata.json\"\n",
    "\n",
    "    with open(metadata_file_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        keys = metadata.keys()\n",
    "\n",
    "        if \"unused_files\" in keys and len(metadata[\"unused_files\"]) > 0:\n",
    "            unparsable_rdf = list()\n",
    "            unparsable_other = list()\n",
    "\n",
    "            for file in metadata[\"unused_files\"]:\n",
    "                file_with_path = f\"{folder}/{dataset}/{file}\"\n",
    "\n",
    "                if check_if_file_name_is_rdf(file):\n",
    "                    unparsable_rdf.append(file)\n",
    "                else:\n",
    "                    unparsable_other.append(file)\n",
    "            \n",
    "            datasets_with_unused_files.append([dataset, unparsable_rdf, unparsable_other])\n",
    "\n",
    "print(\"{:<10}{:<300}{}\".format(\"ID\", \"RDF UNPARSABLE\", \"OTHER UNPARSABLE\"))\n",
    "for d in datasets_with_unused_files:\n",
    "    print(\"{:<10}{:<300}{}\".format(d[0], str(d[1]), str(d[2])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing non parsed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_used_files = 0\n",
    "\n",
    "rdf_files_unused = list()       # contains the path of unused RDF files\n",
    "other_files_unused = list()     # contains the path on unused NON RDF files\n",
    "\n",
    "for dataset in datasets:\n",
    "    metadata_file_path = f\"{folder}/{dataset}/metadata.json\"\n",
    "\n",
    "    with open(metadata_file_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        keys = metadata.keys()\n",
    "\n",
    "        if \"used_files\" in keys and len(metadata[\"used_files\"]) > 0:\n",
    "            total_used_files += len(metadata[\"used_files\"])\n",
    "\n",
    "        if \"unused_files\" in keys and len(metadata[\"unused_files\"]) > 0:\n",
    "\n",
    "            for file in metadata[\"unused_files\"]:\n",
    "                file_with_path = f\"{folder}/{dataset}/{file}\"\n",
    "\n",
    "                if check_if_file_name_is_rdf(file):\n",
    "                    rdf_files_unused.append(file_with_path)\n",
    "                else:\n",
    "                    other_files_unused.append(file_with_path)\n",
    "\n",
    "print(f\"Total used files: {total_used_files}\")\n",
    "print(f\"RDF unusable files: {len(rdf_files_unused)}\")\n",
    "print(f\"NON RDF unusable files: {len(other_files_unused)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths of RDF files that needs to be checked (some of them contains errors, such as spaces in the IRI (very common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_files_unused"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of unused - NON RDF files "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extract archives and zip files using `tarfile` and `zipfile` modules [StackOverflow](https://stackoverflow.com/questions/35690072/how-to-check-if-it-is-a-file-or-folder-for-an-archive-in-python)\n",
    "\n",
    "The [magic number signature](https://en.wikipedia.org/wiki/List_of_file_signatures) can hint file type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import magic\n",
    "import zipfile\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_gz_file(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return f.read(2) == b\"\\x1f\\x8b\"\n",
    "\n",
    "\n",
    "def is_bz2_file(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return f.read(3) == b\"\\x42\\x5a\\x68\"\n",
    "\n",
    "\n",
    "def maybe_rdf(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        try:\n",
    "            return \"<rdf:RDF\" in f.read()\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "\n",
    "def maybe_html(filepath):\n",
    "    \"<!doctype html\"\n",
    "\n",
    "    mt = magic.from_file(filepath).lower()\n",
    "    if \"html\" in mt:\n",
    "        return True\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        try:\n",
    "            return \"<!doctype html\" in f.read().lower()\n",
    "        except Exception:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archives = list()\n",
    "probably_html = list()\n",
    "maybe_can_cast_to_rdf = list()\n",
    "other = list()\n",
    "\n",
    "for file in other_files_unused:\n",
    "    if (\n",
    "        tarfile.is_tarfile(file)\n",
    "        or zipfile.is_zipfile(file)\n",
    "        or is_gz_file(file)\n",
    "        or is_bz2_file(file)\n",
    "    ):\n",
    "        archives.append(file)\n",
    "\n",
    "    elif maybe_html(file):\n",
    "        probably_html.append(file)\n",
    "\n",
    "    elif maybe_rdf(file):\n",
    "        maybe_can_cast_to_rdf.append(file)\n",
    "\n",
    "    else:\n",
    "        other.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of files recognized as archives: {len(archives)}\")\n",
    "print(f\"Total number of files that probably are HTML documents: {len(probably_html)}\")\n",
    "print(f\"Total number of files that probably can be casted to RDF: {len(maybe_can_cast_to_rdf)}\")\n",
    "print(f\"Total number of unknown files: {len(other)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in archives:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in probably_html:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in maybe_can_cast_to_rdf:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in other:\n",
    "    print(file, magic.from_file(file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
