{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats\n",
    "Statistics about downloaded and processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the relative path or the absolute path pointing to the directory in which datasets have been downloaded\n",
    "folder = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_datasets = list()  # completely downloaded\n",
    "partial_datasets = list()  # not completely downloaded / parsed (at least 1 valid file)\n",
    "empty_datasets = list()  # only metadata for these datasets\n",
    "not_processed = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan the directory containing the downloaded datasets\n",
    "datasets = sorted(os.listdir(folder), key=lambda i: int(i))\n",
    "total_datasets = len(datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SIZE_LIMIT = 100 * 1024 * 1024  # 100 MB\n",
    "\n",
    "\n",
    "def is_file_larger_than_size_limit(filepath: str) -> bool:\n",
    "    size = os.path.getsize(str(filepath))\n",
    "    return int(size) >= int(SIZE_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDF_SUFFIXES = [\"rdf\", \"ttl\", \"owl\", \"n3\", \"nt\", \"jsonld\", \"nq\", \"trig\", \"trix\"]\n",
    "\n",
    "\n",
    "def check_if_file_name_is_rdf(name: str) -> bool:\n",
    "    return name.split(\".\")[-1] in RDF_SUFFIXES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import magic\n",
    "\n",
    "\n",
    "def is_html(filepath: str) -> bool:\n",
    "    mt = magic.from_file(filepath).lower()\n",
    "    if \"html\" in mt:\n",
    "        return True\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        try:\n",
    "            return \"<!doctype html\" in f.read().lower()\n",
    "        except Exception:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def is_json(filepath: str) -> bool:\n",
    "    with open(filepath, \"r\") as f:\n",
    "        try:\n",
    "            json.load(f)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_file(file_path: str):\n",
    "    if os.path.isfile(file_path):\n",
    "        print(f\"Deleting {file_path}\")\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the processing status for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    EMPTY = 0\n",
    "    NOT_PROCESSED = 1\n",
    "    PARTIAL = 2\n",
    "    COMPLETE = 3\n",
    "\n",
    "\n",
    "def analyze_dataset(dataset_path) -> DatasetType:\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        keys = metadata.keys()\n",
    "\n",
    "        # check if the dataset has been mined through the data extractor\n",
    "        if not \"unused_files\" in keys:\n",
    "            return DatasetType.NOT_PROCESSED\n",
    "\n",
    "        # check if the dataset has been downloaded completely\n",
    "        completely_downloaded = False\n",
    "        if \"failed_download_urls\" in keys:\n",
    "            completely_downloaded = len(metadata[\"failed_download_urls\"]) == 0\n",
    "\n",
    "        # check if the file dataset contains at least one file that has been parsed\n",
    "        contains_a_valid_file = len(metadata[\"used_files\"]) > 0\n",
    "\n",
    "        # check if the dataset has some files that have not been parsed or has thrown errors while parsing\n",
    "        error_while_parsing = len(metadata[\"unused_files\"]) == 0\n",
    "\n",
    "        \"\"\" \n",
    "        A dataset is complete only if all these conditions are satisfied:\n",
    "        1) has been completely downloaded\n",
    "        2) contains at least one valid file (>0)\n",
    "        3) no file has generated error while parsing\n",
    "        \"\"\"\n",
    "\n",
    "        if completely_downloaded and contains_a_valid_file and not error_while_parsing:\n",
    "            return DatasetType.COMPLETE\n",
    "\n",
    "        \"\"\"\n",
    "        A dataset is partial if:\n",
    "        1) contains at least one valid file (>0)\n",
    "        2) some files may not have been downloaded\n",
    "        3) some files may have generated errors or not being the correct type to be used\n",
    "        \"\"\"\n",
    "\n",
    "        if contains_a_valid_file:\n",
    "            return DatasetType.PARTIAL\n",
    "\n",
    "        \"\"\"\n",
    "        If a dataset doesn't contain any file\n",
    "        \"\"\"\n",
    "        return DatasetType.EMPTY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    metadata_file_path = f\"{folder}/{dataset}/metadata.json\"\n",
    "\n",
    "    res = analyze_dataset(metadata_file_path)\n",
    "\n",
    "    if res == DatasetType.COMPLETE:\n",
    "        complete_datasets.append(dataset)\n",
    "\n",
    "    if res == DatasetType.PARTIAL:\n",
    "        partial_datasets.append(dataset)\n",
    "\n",
    "    if res == DatasetType.EMPTY:\n",
    "        empty_datasets.append(dataset)\n",
    "\n",
    "    if res == DatasetType.NOT_PROCESSED:\n",
    "        not_processed.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of datasets: 31589\n",
      "Complete datasets: 8\n",
      "Partial datasets: 26377\n",
      "Empty datasets: 5204\n",
      "Not processed datasets: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of datasets: {total_datasets}\")\n",
    "print(f\"Complete datasets: {len(complete_datasets)}\")   # completely downloaded and parsed\n",
    "print(f\"Partial datasets: {len(partial_datasets)}\")\n",
    "print(f\"Empty datasets: {len(empty_datasets)}\")\n",
    "print(f\"Not processed datasets: {len(not_processed)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List datasets with unused file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_with_unused_files = list()\n",
    "\n",
    "for dataset in datasets:\n",
    "    metadata_file_path = f\"{folder}/{dataset}/metadata.json\"\n",
    "\n",
    "    with open(metadata_file_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        keys = metadata.keys()\n",
    "\n",
    "        if \"unused_files\" in keys and len(metadata[\"unused_files\"]) > 0:\n",
    "            unparsable_rdf = list()\n",
    "            unparsable_other = list()\n",
    "\n",
    "            for file in metadata[\"unused_files\"]:\n",
    "                file_with_path = f\"{folder}/{dataset}/{file}\"\n",
    "\n",
    "                if check_if_file_name_is_rdf(file):\n",
    "                    unparsable_rdf.append(file)\n",
    "                else:\n",
    "                    unparsable_other.append(file)\n",
    "            \n",
    "            datasets_with_unused_files.append([dataset, unparsable_rdf, unparsable_other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Dataset ID | RDF not parsable | Other not parsable |\n",
       "| --- | --- | --- |\n",
       "| 2 | ['ppg-sf-dump.rdf'] | [] |\n",
       "| 6 | ['ppg-sf-dump.rdf'] | [] |\n",
       "| 11580 | ['rows.rdf'] | [] |\n",
       "| 13263 | ['Govwild_rdf.n3'] | [] |\n",
       "| 13283 | ['txn-images.ttl', 'txn-distribution.ttl'] | [] |\n",
       "| 13284 | ['deck.rdf'] | [] |\n",
       "| 13347 | ['geospecies.rdf'] | [] |\n",
       "| 13368 | ['all-geonames.rdf'] | [] |\n",
       "| 13378 | ['wordnet-partmeronym.rdf', 'wordnet-substancemeronym.rdf', 'wordnet-seealso.rdf', 'wordnet-pertainsto.rdf', 'wordnet-causes.rdf', 'wordnet-sameverbgroupas.rdf', 'wordnet-hyponym.rdf', 'wordnet-classifiedby.rdf', 'wordnet-membermeronym.rdf', 'wordnet-participleof.rdf', 'wordnet-entailment.rdf', 'wordnet-similarity.rdf', 'wordnet-antonym.rdf', 'wordnet-attribute.rdf', 'wordnet-derivationallyrelated.rdf'] | [] |\n",
       "| 13565 | ['download-20120123.rdf'] | [] |\n",
       "| 13997 | ['PERSEE_align_wikipedia_2021-09-24.rdf', 'PERSEE_align_ORCID_2021-09-24.rdf', 'PERSEE_align_Idref_2021-09-24.rdf', 'PERSEE_align_All_2021-09-24.rdf', 'PERSEE_align_DBpediaFR_2021-09-24.rdf', 'PERSEE_align_Bnf_2021-09-24.rdf', 'PERSEE_align_Isni_2021-09-24.rdf', 'PERSEE_align_idHAL_2021-09-24.rdf', 'PERSEE_align_viaf_2021-09-24.rdf', 'PERSEE_align_RePEc_2021-09-24.rdf', 'PERSEE_align_wikidata_2021-09-24.rdf', 'PERSEE_align_wikipediaFR_2021-09-24.rdf', 'PERSEE_align_DBpedia_2021-09-24.rdf'] | ['persee-person-align-rdf.tar.gz', 'license.txt'] |\n",
       "| 14054 | ['rdf.rdf'] | [] |\n",
       "| 14079 | ['eat.nt', 'wheat.rdf'] | [] |\n",
       "| 14252 | ['wordnet-partmeronym.rdf', 'wordnet-substancemeronym.rdf', 'wordnet-seealso.rdf', 'wordnet-pertainsto.rdf', 'wordnet-causes.rdf', 'wordnet-sameverbgroupas.rdf', 'wordnet-hyponym.rdf', 'wordnet-classifiedby.rdf', 'wordnet-membermeronym.rdf', 'wordnet-participleof.rdf', 'wordnet-entailment.rdf', 'wordnet-similarity.rdf', 'wordnet-antonym.rdf', 'wordnet-attribute.rdf', 'wordnet-derivationallyrelated.rdf'] | [] |\n",
       "| 14277 | ['txn-images.ttl', 'txn-distribution.ttl'] | [] |\n",
       "| 14324 | ['geospecies.rdf'] | [] |\n",
       "| 14344 | ['all-geonames.rdf'] | [] |\n",
       "| 14364 | ['Govwild_rdf.n3'] | [] |\n",
       "| 14417 | ['deck.rdf'] | [] |\n",
       "| 15243 | ['en.rdf', 'fr.rdf'] | [] |\n",
       "| 21023 | ['2016-allievi-partecipanti.nt'] | [] |\n",
       "| 21532 | ['jrcnames_uri.nt'] | [] |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "markdown_table = \"\"\"\n",
    "| Dataset ID | RDF not parsable | Other not parsable |\n",
    "| --- | --- | --- |\n",
    "\"\"\"\n",
    "\n",
    "for d in datasets_with_unused_files:\n",
    "    markdown_table += (\"| {} | {} | {} |\\n\".format(d[0], str(d[1]), str(d[2])))\n",
    "\n",
    "display(Markdown(markdown_table))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing unused files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total used files: 27886\n",
      "Big files: 15\n",
      "Unusable files: 53\n"
     ]
    }
   ],
   "source": [
    "total_used_files = 0\n",
    "big_files = list()\n",
    "unusable_files = list()\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset_folder_path = f\"{folder}/{dataset}\"\n",
    "    metadata_file_path = f\"{dataset_folder_path}/metadata.json\"\n",
    "\n",
    "    with open(metadata_file_path, \"r\") as f:\n",
    "        metadata = json.load(f, strict=False)\n",
    "\n",
    "        keys = metadata.keys()\n",
    "\n",
    "        if \"used_files\" in keys and len(metadata[\"used_files\"]) > 0:\n",
    "            total_used_files += len(metadata[\"used_files\"])\n",
    "\n",
    "        if \"unused_files\" in keys and len(metadata[\"unused_files\"]) > 0:\n",
    "            for uf in metadata[\"unused_files\"]:\n",
    "                file_path = f\"{dataset_folder_path}/{uf}\"\n",
    "                if is_file_larger_than_size_limit(file_path):\n",
    "                    big_files.append(file_path)\n",
    "                else:\n",
    "                    unusable_files.append(file_path)\n",
    "\n",
    "print(f\"Total used files: {total_used_files}\")\n",
    "print(f\"Big files: {len(big_files)}\")\n",
    "print(f\"Unusable files: {len(unusable_files)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/13283/txn-images.ttl',\n",
       " 'datasets/13283/txn-distribution.ttl',\n",
       " 'datasets/13284/deck.rdf',\n",
       " 'datasets/13378/wordnet-partmeronym.rdf',\n",
       " 'datasets/13378/wordnet-substancemeronym.rdf',\n",
       " 'datasets/13378/wordnet-seealso.rdf',\n",
       " 'datasets/13378/wordnet-pertainsto.rdf',\n",
       " 'datasets/13378/wordnet-causes.rdf',\n",
       " 'datasets/13378/wordnet-sameverbgroupas.rdf',\n",
       " 'datasets/13378/wordnet-hyponym.rdf',\n",
       " 'datasets/13378/wordnet-classifiedby.rdf',\n",
       " 'datasets/13378/wordnet-membermeronym.rdf',\n",
       " 'datasets/13378/wordnet-participleof.rdf',\n",
       " 'datasets/13378/wordnet-entailment.rdf',\n",
       " 'datasets/13378/wordnet-similarity.rdf',\n",
       " 'datasets/13378/wordnet-antonym.rdf',\n",
       " 'datasets/13378/wordnet-attribute.rdf',\n",
       " 'datasets/13378/wordnet-derivationallyrelated.rdf',\n",
       " 'datasets/13997/persee-person-align-rdf.tar.gz',\n",
       " 'datasets/13997/license.txt',\n",
       " 'datasets/13997/PERSEE_align_wikipedia_2021-09-24.rdf',\n",
       " 'datasets/13997/PERSEE_align_ORCID_2021-09-24.rdf',\n",
       " 'datasets/13997/PERSEE_align_Idref_2021-09-24.rdf',\n",
       " 'datasets/13997/PERSEE_align_All_2021-09-24.rdf',\n",
       " 'datasets/13997/PERSEE_align_DBpediaFR_2021-09-24.rdf',\n",
       " 'datasets/13997/PERSEE_align_Bnf_2021-09-24.rdf',\n",
       " 'datasets/13997/PERSEE_align_Isni_2021-09-24.rdf',\n",
       " 'datasets/13997/PERSEE_align_idHAL_2021-09-24.rdf',\n",
       " 'datasets/13997/PERSEE_align_viaf_2021-09-24.rdf',\n",
       " 'datasets/13997/PERSEE_align_RePEc_2021-09-24.rdf',\n",
       " 'datasets/13997/PERSEE_align_wikidata_2021-09-24.rdf',\n",
       " 'datasets/13997/PERSEE_align_wikipediaFR_2021-09-24.rdf',\n",
       " 'datasets/13997/PERSEE_align_DBpedia_2021-09-24.rdf',\n",
       " 'datasets/14054/rdf.rdf',\n",
       " 'datasets/14079/wheat.rdf',\n",
       " 'datasets/14252/wordnet-partmeronym.rdf',\n",
       " 'datasets/14252/wordnet-substancemeronym.rdf',\n",
       " 'datasets/14252/wordnet-seealso.rdf',\n",
       " 'datasets/14252/wordnet-pertainsto.rdf',\n",
       " 'datasets/14252/wordnet-causes.rdf',\n",
       " 'datasets/14252/wordnet-sameverbgroupas.rdf',\n",
       " 'datasets/14252/wordnet-hyponym.rdf',\n",
       " 'datasets/14252/wordnet-classifiedby.rdf',\n",
       " 'datasets/14252/wordnet-membermeronym.rdf',\n",
       " 'datasets/14252/wordnet-participleof.rdf',\n",
       " 'datasets/14252/wordnet-entailment.rdf',\n",
       " 'datasets/14252/wordnet-similarity.rdf',\n",
       " 'datasets/14252/wordnet-antonym.rdf',\n",
       " 'datasets/14252/wordnet-attribute.rdf',\n",
       " 'datasets/14252/wordnet-derivationallyrelated.rdf',\n",
       " 'datasets/14277/txn-images.ttl',\n",
       " 'datasets/14277/txn-distribution.ttl',\n",
       " 'datasets/14417/deck.rdf']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unusable_files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/2/ppg-sf-dump.rdf',\n",
       " 'datasets/6/ppg-sf-dump.rdf',\n",
       " 'datasets/11580/rows.rdf',\n",
       " 'datasets/13263/Govwild_rdf.n3',\n",
       " 'datasets/13347/geospecies.rdf',\n",
       " 'datasets/13368/all-geonames.rdf',\n",
       " 'datasets/13565/download-20120123.rdf',\n",
       " 'datasets/14079/eat.nt',\n",
       " 'datasets/14324/geospecies.rdf',\n",
       " 'datasets/14344/all-geonames.rdf',\n",
       " 'datasets/14364/Govwild_rdf.n3',\n",
       " 'datasets/15243/en.rdf',\n",
       " 'datasets/15243/fr.rdf',\n",
       " 'datasets/21023/2016-allievi-partecipanti.nt',\n",
       " 'datasets/21532/jrcnames_uri.nt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "diff datasets/2/ppg-sf-dump.rdf datasets/6/ppg-sf-dump.rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "diff datasets/13263/Govwild_rdf.n3 datasets/14364/Govwild_rdf.n3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "diff datasets/13347/geospecies.rdf datasets/14324/geospecies.rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cmp datasets/13368/all-geonames.rdf datasets/14344/all-geonames.rdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicates\n",
    "- `datasets/2/ppg-sf-dump.rdf` is the same file as `datasets/6/ppg-sf-dump.rdf`\n",
    "- `datasets/13263/Govwild_rdf.n3` is the same file as `datasets/14364/Govwild_rdf.n3`\n",
    "- `datasets/13347/geospecies.rdf` is the same file as `datasets/14324/geospecies.rdf`\n",
    "- `datasets/13368/all-geonames.rdf` is the same file as `datasets/14344/all-geonames.rdf`\n",
    "\n",
    "#### Invalid files\n",
    "While importing in GraphDB the files below generated errors\n",
    "- `datasets/13263/Govwild_rdf.n3` contains syntax error, GraphDB raises `org.eclipse.rdf4j.sail.SailException: Invalid IRI value`\n",
    "- `datasets/13347/geospecies.rdf` contains syntax error, GraphDB raises `org.eclipse.rdf4j.sail.SailException: Invalid IRI value`\n",
    "- `datasets/13368/all-geonames.rdf` is not processable, GraphDB raises `RDF parse error: content is not allowed in prolog`\n",
    "- `datasets/13565/download-20120123.rdf` contains syntax error, GraphDB raises `org.eclipse.rdf4j.sail.SailException: Invalid IRI value`\n",
    "- `datasets/15243/fr.rdf` contains syntax error, GraphDB raises `RDF parse error`\n",
    "- `datasets/21532/jrcnames_uri.nt` contains syntax error, GraphDB raises `org.eclipse.rdf4j.sail.SailException: Invalid IRI value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_valid_big_files = [\n",
    "    \"datasets/2/ppg-sf-dump.rdf\",\n",
    "    \"datasets/11580/rows.rdf\",\n",
    "    \"datasets/14079/eat.nt\",\n",
    "    \"datasets/15243/en.rdf\",\n",
    "    \"datasets/21023/2016-allievi-partecipanti.nt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting datasets/6/ppg-sf-dump.rdf\n",
      "Deleting datasets/13263/Govwild_rdf.n3\n",
      "Deleting datasets/13347/geospecies.rdf\n",
      "Deleting datasets/13368/all-geonames.rdf\n",
      "Deleting datasets/13565/download-20120123.rdf\n",
      "Deleting datasets/14324/geospecies.rdf\n",
      "Deleting datasets/14344/all-geonames.rdf\n",
      "Deleting datasets/14364/Govwild_rdf.n3\n",
      "Deleting datasets/15243/fr.rdf\n",
      "Deleting datasets/21532/jrcnames_uri.nt\n"
     ]
    }
   ],
   "source": [
    "for file in big_files:\n",
    "    if file not in distinct_valid_big_files:\n",
    "        delete_file(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
