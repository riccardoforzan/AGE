{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rdflib import Graph\n",
    "import languagemodels as lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"datasets/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "usable_files = list()\n",
    "\n",
    "with open(f\"{dataset_path}/metadata.json\") as mf:\n",
    "    metadata = json.load(mf)\n",
    "    for file in metadata[\"extracted\"]:\n",
    "        if file[\"extractedWith\"] == \"RDFLib\":\n",
    "            usable_files.append(f\"{dataset_path}/{file['file']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=Nab05f0ee12df4d9dbe7f8169bfc61ff0 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Graph()\n",
    "graph.parse(usable_files[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1 - Process the graph as triples and infer context using LM\n",
    "\n",
    "This provides acceptable results, the string generated by the LM seems related to the dataset.\n",
    "This approach depends on:\n",
    "- The model used\n",
    "- Very sensitive to minor prompt changes\n",
    "- You need to provide some **good** text that is not too long and that is \"significant\", it does not lead to a good result if you provide more text, and allucinate a lot if you provide only a couple of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_cleaner(input: str) -> str:\n",
    "    return input.split(\"/\")[-1].replace(\"#\", \"\").replace(\".\", \" \").replace(\"_\", \" \")\n",
    "\n",
    "def is_majority_numbers(string: str) -> str:\n",
    "    if not string:\n",
    "        return False\n",
    "    numbers_count = sum(char.isdigit() for char in string)\n",
    "    percentage = numbers_count / len(string)\n",
    "    return percentage > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "\n",
    "for subj, pred, obj in graph:\n",
    "    cs = string_cleaner(subj)\n",
    "    cp = string_cleaner(pred)\n",
    "    co = string_cleaner(obj)\n",
    "\n",
    "    if not is_majority_numbers(cs) and not is_majority_numbers(co):\n",
    "        sentences.append(f\"{cs} {cp} {co}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['curso sf dump rdf source ',\n",
       " 'curso sf dump rdf voiddataDump curso sf dump rdf',\n",
       " 'curso sf dump rdf voiddataDump ies sf dump rdf',\n",
       " 'curso sf dump rdf title Lista de cursos de pós-graduação - 2017',\n",
       " 'curso sf dump rdf title List of postgraduate courses - 2017',\n",
       " 'curso sf dump rdf 22-rdf-syntax-nstype Dataset',\n",
       " 'curso sf dump rdf publisher capes gov br',\n",
       " 'curso sf dump rdf creator capes gov br',\n",
       " 'curso sf dump rdf 22-rdf-syntax-nstype voidDataset',\n",
       " 'curso sf dump rdf voiddataDump pessoa sf dump rdf',\n",
       " 'curso sf dump rdf voiddataDump ppg sf dump rdf',\n",
       " 'curso sf dump rdf description List of postgraduate courses as in 2017, published in Dados Abertos website of CAPES',\n",
       " 'curso sf dump rdf homepage dataset',\n",
       " 'curso sf dump rdf description Lista de cursos de pós-graduação ativos em 2017 publicada no website Dados Abertos, da CAPES']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_string = ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract context from the following text:\n",
      "curso sf dump rdf source  curso sf dump rdf voiddataDump curso sf dump rdf curso sf dump rdf voiddataDump ies sf dump rdf curso sf dump rdf title Lista de cursos de pós-graduação - 2017 curso sf dump rdf title List of postgraduate courses - 2017 curso sf dump rdf 22-rdf-syntax-nstype Dataset curso sf dump rdf publisher capes gov br curso sf dump rdf creator capes gov br curso sf dump rdf 22-rdf-syntax-nstype voidDataset curso sf dump rdf voiddataDump pessoa sf dump rdf curso sf dump rdf voiddataDump ppg sf dump rdf curso sf dump rdf description List of postgraduate courses as in 2017, published in Dados Abertos website of CAPES curso sf dump rdf homepage dataset curso sf dump rdf description Lista de cursos de pós-graduação ativos em 2017 publicada no website Dados Abertos, da CAPES\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Extract context from the following text:\\n{combined_string}\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The text provides information about the list of postgraduate courses published in Dados Abertos website of CAPES, including their source, publisher, creator, and description.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.do(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChatGTP 3.5 result with the same exact prompt**\n",
    "\n",
    "From the provided text, we can extract the following context:\n",
    "- There is a course related to RDF (Resource Description Framework) and data dumping.\n",
    "- The course seems to be associated with CAPES, a governmental organization (as indicated by the \"gov.br\" domain).\n",
    "- The course is related to postgraduate education and specifically focuses on a list of postgraduate courses from 2017.\n",
    "- The list of postgraduate courses is published on the Dados Abertos (Open Data) website of CAPES.\n",
    "- There is a publisher mentioned as \"capes.gov.br\" for the RDF dump related to the course.\n",
    "- The course's title is \"List of postgraduate courses - 2017\" or \"Lista de cursos de pós-graduação - 2017\" in Portuguese.\n",
    "- The course might involve void datasets and the 22-rdf-syntax-ns type.\n",
    "- There is a mention of \"pessoa\" (person) as an RDF dump source, but it is unclear how it relates to the course."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2 - Integrate NetworkX\n",
    "\n",
    "Trying to extract top nodes using graph structure to create a representation of the graph\n",
    "\n",
    "**INFEASIBLE** takes over 30 minutes only to compute `betweenness_centrality`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from rdflib.extras.external_graph_libs import rdflib_to_networkx_graph\n",
    "nx_graph = rdflib_to_networkx_graph(graph)\n",
    "\n",
    "# Degree centrality\n",
    "degree_centrality = nx.degree_centrality(nx_graph)\n",
    "sorted_degree_centrality = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Betweenness centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(nx_graph)\n",
    "sorted_betweenness_centrality = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "edge_betweenness = nx.edge_betweenness_centrality(nx_graph)\n",
    "sorted_edge_betweenness = sorted(edge_betweenness.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Eigenvector centrality\n",
    "eigenvector_centrality = nx.eigenvector_centrality(nx_graph)\n",
    "sorted_eigenvector_centrality = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Degree Centrality:\")\n",
    "for node, centrality in sorted_degree_centrality:\n",
    "    print(f\"Node {node}: {centrality}\")\n",
    "\n",
    "print(\"Betweenness Centrality:\")\n",
    "for node, centrality in sorted_betweenness_centrality:\n",
    "    print(f\"Node {node}: {centrality}\")\n",
    "\n",
    "print(\"Edge Betweenness Centrality:\")\n",
    "for edge, centrality in sorted_edge_betweenness:\n",
    "    print(f\"Edge {edge}: {centrality}\")\n",
    "\n",
    "print(\"\\nEigenvector Centrality:\")\n",
    "for node, centrality in sorted_eigenvector_centrality:\n",
    "    print(f\"Node {node}: {centrality}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3 - RDF Queries\n",
    "\n",
    "Trying to construct a reduced ontology that shows connection between classes. It would have been used as input to the LM for trying to inferring the context of the dataset.\n",
    "\n",
    "**INFEASIBLE: PREFIXES NEEDED FOR THE QUERY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "\n",
    "\n",
    "def clean_string(s: str) -> str:\n",
    "    return \" \".join(s.split()).encode(\"unicode_escape\").decode(\"unicode_escape\")\n",
    "\n",
    "\n",
    "def get_classes(graph) -> list:\n",
    "    q = \"\"\"\n",
    "    SELECT DISTINCT ?class\n",
    "    WHERE {\n",
    "        ?s a ?class .\n",
    "    }\n",
    "    \"\"\"\n",
    "    match = graph.query(q)\n",
    "\n",
    "    classes = list()\n",
    "\n",
    "    for item in match:\n",
    "        label = clean_string(str(item[0]))\n",
    "        classes.append(label)\n",
    "\n",
    "    return classes\n",
    "\n",
    "\n",
    "def get_connected_info(graph, cls: str) -> dict:\n",
    "    q = Template(\n",
    "        \"\"\"\n",
    "    SELECT DISTINCT ?p ?class\n",
    "    WHERE {\n",
    "        ?s a ov:$cls .\n",
    "        ?s ?p ?o .\n",
    "        ?o a ?class .\n",
    "    }\n",
    "    \"\"\"\n",
    "    ).safe_substitute(cls=cls)\n",
    "\n",
    "    match = graph.query(q)\n",
    "\n",
    "    res = list()\n",
    "\n",
    "    for item in match:\n",
    "        res.append((clean_string(str(item[0])), clean_string(str(item[1]))))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = get_classes(graph)\n",
    "\n",
    "for c in classes:\n",
    "    print(get_connected_info(graph, c))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
